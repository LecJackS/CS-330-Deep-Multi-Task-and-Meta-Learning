{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 330 Autumn 2019/2020 Homework 3\n",
    "\n",
    "## Goal Conditioned Reinforcement Learning and Hindsight Experience Replay\n",
    "\n",
    "Due Wednesday November 6th, 11:59 PM PST\n",
    "\n",
    "SUNet ID:\n",
    "Name:\n",
    "Collaborators:\n",
    "\n",
    "By turning in this assignment, I agree by the Stanford honor code and declare that all of\n",
    "this is my own work.\n",
    "\n",
    "# Overview\n",
    "\n",
    "In this assignment we will be looking at goal-conditioned learning and hindsight experience\n",
    "replay (HER). In particular, you will:\n",
    "\n",
    "1. Adapt an existing model (a Deep Q-Network) to be goal-conditioned.\n",
    "2. Run goal-conditioned DQN on two environments\n",
    "3. Implement Hindsight Experience Replay (HER) [1,2] on top of a goal-conditioned DQN\n",
    "for each environment\n",
    "4. Compare the performance with and without HER\n",
    "\n",
    "**Submission**: To submit your homework, submit one pdf report and one zip file to Gradescope, where the report will contain answers to the deliverables listed below and the zip file contains your code bits_main.py and sawyer_main.py with the filled in solutions.\n",
    "\n",
    "## Code Overview:\n",
    "\n",
    "The code consists of four files. You should only make modifications to\n",
    "two of them.\n",
    "\n",
    "* `BitFlip.py`: Bit flipping environment for problems 1-3. You should not modify this\n",
    "file, though it may be helpful to look at it and understand the environment.\n",
    "\n",
    "\n",
    "* `buffers.py`: Buffers for storing experiences. You should not modify this file, though\n",
    "it may be useful to look it.\n",
    "\n",
    "\n",
    "* `bits_main.py`: Main loop and helper functions for solving the bit flipping environment.\n",
    "You will add your solutions to problems 1 and 2 to this file and run it for problem 3.\n",
    "\n",
    "* `sawyer_main.py`: Main loop and helper functions for solving the Sawyer arm environment. You will add your solutions to problem 4 to this file and run it for problem 5.\n",
    "\n",
    "\n",
    "## Dependencies:\n",
    "\n",
    "We expect code in Python 3.5+ with Pillow, scipy, numpy, tensorflow,\n",
    "gym, mujoco, multiworld installed.\n",
    "\n",
    "> `pip install pillow, scipy, numpy, gym, mujoco, multiworld, tensorflow`\n",
    "\n",
    "\n",
    "## Environments\n",
    "\n",
    "You will be running on two environments:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment 1: Bit Flipping Environment\n",
    "\n",
    "In the bit-flipping environment, **the state is a binary vector with length n**.\n",
    "\n",
    "The **goal** is **to reach a known goal vector**, which is also a binary vector with length n.\n",
    "\n",
    "At each step, **we can flip a single value** in the vector (changing a 0 to 1 or a 1 to 0).\n",
    "\n",
    "This environment can very easily be solved without reinforcement learning, but we will use a DQN to demonstrate how adding HER can improve performance.\n",
    "\n",
    "![asd](../img/hw3-flip-env.png)\n",
    "\\* Diagram: 4 bit environment\n",
    "\n",
    "The bit flipping environment is an example of an environment with **sparse rewards**.\n",
    "\n",
    "At each step, **we receive a reward of -1 when the goal and state vector do not match and a reward\n",
    "of 0 when they do**.\n",
    "\n",
    "As the **size of the vector grows**, we receive **fewer and fewer non-negative rewards**.\n",
    "\n",
    "**Adding HER helps us train in an environment with sparse rewards** (more details later).\n",
    "\n",
    "The bit flipping environment is included in the homework zip file as `BitFlip.py` and does\n",
    "not require additional installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment 2: 2D Sawyer Arm\n",
    "\n",
    "The Sawyer Arm is a multi-jointed robotic arm for grasping and reaching (https://robots.ieee.org/robots/sawyer/).\n",
    "\n",
    "The arm operates in a 2D space, and the **goal is to move the robot to a set of coordinates (x, y)**.\n",
    "\n",
    "To run the Sawyer Arm environment, you will have to install several packages.\n",
    "\n",
    "If you have any trouble with installation, please post on Piazza; itâ€™s likely that other students have\n",
    "run into the same problem.\n",
    "\n",
    "### Installing gym\n",
    "\n",
    "Gym is a package for comparing reinforcement learning algorithms.\n",
    "\n",
    "It is a widely used library, and contains several simulated tasks for testing algorithms such as teaching robots to walk and balancing a pole on a cart. Instructions for installing gym are here http://gym.openai.com/docs/.\n",
    "\n",
    "Run:\n",
    "\n",
    "    pip install gym\n",
    "\n",
    "### Installing Mujoco\n",
    "\n",
    "Mujoco is a physics simulation engine that runs with gym. You will be emailed a class license for Mujoco.\n",
    "\n",
    "DO NOT REDISTRIBUTE THE LICENSE KEY.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "for installing Mujoco can be found here: https://github.com/openai/mujoco-py.\n",
    "\n",
    "Run:\n",
    "\n",
    "    pip install mujoco-py\n",
    "\n",
    "### Installing multiworld\n",
    "    \n",
    "To install multiworld, run\n",
    "\n",
    "    git clone https://github.com/vitchyr/multiworld\n",
    "    \n",
    "navigate to the setup.py and run\n",
    "    \n",
    "    python setup.py install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Implementing Goal-Conditioned RL on Bit Flipping\n",
    "\n",
    "The file `bits_main.py` contains a DQN that runs in a bit-flipping environment.\n",
    "\n",
    "Currently the DQN is **not goal-conditioned** and does **not have HER implemented**.\n",
    "\n",
    "The Q-function takes in **only the state as the input**, and does not consider the goal.\n",
    "\n",
    "To modify the Q-function to be goal-conditioned, **concatenate the observation vector with the goal vector and pass the combined vector to the Q-function**.\n",
    "\n",
    "You can think of the goal-conditioned implementation as an **extended Markov decision process (MDP)**, where your **state space contains both the original state and the goal**.\n",
    "\n",
    "a) Run `bits_main.py` with the following arguments:\n",
    "\n",
    "    python bits_main.py --num_bits=7 --num_epochs=150\n",
    "    \n",
    "**Save the generated plot** and include it in your homework.\n",
    "\n",
    "This plot illustrates the performance without goal conditioning.\n",
    "\n",
    "b) **Modify the DQN** so that it is **goal-conditioned**.\n",
    "\n",
    "The places where you will need to make modifications are marked in `bits_main.py`.\n",
    "\n",
    "You should **not** make modifications to `buffers.py` or `BitFlip.py`.\n",
    "\n",
    "*Hint:* the bit flipping environment returns the state and goal vector when reset() is called.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T22:35:23.144361Z",
     "start_time": "2020-08-28T22:34:50.444381Z"
    }
   },
   "source": [
    "a) *Dont forget to run a) and save the plot, you'll modify that code in a bit*\n",
    "\n",
    "    python bits_main.py --num_bits=7 --num_epochs=150\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Particularly, we need to modify TWO sections of the code:\n",
    "\n",
    "1. Input to the NN model\n",
    "2. Solve environment loop\n",
    "\n",
    "> Why do you think we need to modify the input to the neural network model as we modify DQN to be **goal-conditioned**?\n",
    ">\n",
    "> > ```python\n",
    "> > # ======================== TODO modify code ========================\n",
    "> > \n",
    "> > self.inp = tf.placeholder(shape = [None, num_bits],dtype = tf.float32)\n",
    "> > \n",
    "> > # ========================      END TODO       ========================\n",
    "> > net = self.inp\n",
    "> > net = slim.fully_connected(net,hidden_dim,activation_fn = tf.nn.relu)\n",
    "> > ...\n",
    "> > ```\n",
    ">\n",
    "> What about the loop over each episode? what do we need to change there?\n",
    ">\n",
    "> > ```python\n",
    "> > for t in range(num_bits):\n",
    "> >     # attempt to solve the state - number of steps given to solve the\n",
    "> >     # state is equal to the size of the vector\n",
    "> >     \n",
    "> >     # ======================== TODO modify code ========================\n",
    "> > \n",
    "> >     inp_state = state\n",
    "> >     # forward pass to find action\n",
    "> >     action = sess.run(model.predict,feed_dict = {model.inp : [inp_state]})[0]\n",
    "> >     # take the action\n",
    "> >     next_state,reward,done, _ = bit_env.step(action)\n",
    "> >     # add to the episode experience (what happened)\n",
    "> >      episode_experience.append((state, action, reward, next_state, goal_state))\n",
    "> >     # calculate total reward\n",
    "> >     total_reward+=reward\n",
    "> >     # update state\n",
    "> >     state = next_state\n",
    "> >     # mark that we've finished the episode and succeeded with training\n",
    "> >     if done:\n",
    "> >         if succeeded:\n",
    "> >             continue\n",
    "> >         else:\n",
    "> >             succeeded = True\n",
    "> >     \n",
    "> >     # ========================      END TODO       ========================\n",
    "> > ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall:\n",
    "\n",
    "Adding a goal to our framework changes two things:\n",
    "\n",
    "1. **States now also include the goal state** :\n",
    "\n",
    "  ```python\n",
    "   state = [observation, goal_state]\n",
    "   ```\n",
    "    \n",
    "   which in our case those are vectors of n bits/digits\n",
    "   \n",
    "   ```python\n",
    "   observation = [0 0 0 1]\n",
    "   goal_state  = [0 1 0 1]\n",
    "\n",
    "   state = [0 0 0 1 0 1 0 1]\n",
    "   ```\n",
    "\n",
    "2. **Rewards are replaced by a distance metric between observation and goal state**\n",
    "   \n",
    "   Before we had:\n",
    "   \n",
    "   ```python\n",
    "   next_state, reward, done, _ = bit_env.step(action)\n",
    "   ```\n",
    "    \n",
    "   and we want to replace that reward for a distance to a given goal:\n",
    "   \n",
    "   ```python\n",
    "   # Sparse\n",
    "   # -1 if not equal; 0 if equal\n",
    "   # -dirac{ state != goal_state }\n",
    "   reward = -1 if (next_state != goal_state)\n",
    "\n",
    "   # L2 norm (squared): ||different bits||^2\n",
    "   reward = -1 * np.sum(np.power( (next_state - goal_state), 2) )\n",
    "    ```\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to add goal conditioning to our DQN, but without changing the environment, so we need to modify **EACH** state as is observed to include, also, the goal state:\n",
    "\n",
    "> ```python\n",
    "> inp_state = np.concatenate(state, goal_state)\n",
    "> ```\n",
    "\n",
    "Each `state` now (and also `next state`s) will need to have the observed state as a vector concatenated with the goal state vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T00:04:56.818440Z",
     "start_time": "2020-08-29T00:04:54.994775Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import tensorflow.contrib.slim as slim\n",
    "import tf_slim as slim\n",
    "from BitFlip import BitFlipEnv\n",
    "from buffers import Buffer\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string(\"HER\", \"None\",\n",
    "                    \"different strategies of choosing goal. Possible values are :- future, final, episode or None. If None HER is not used\")\n",
    "flags.DEFINE_integer(\"num_bits\", 15, \"number of bits in the bit-flipping environment\")\n",
    "flags.DEFINE_integer(\"num_epochs\", 250, \"Number of epochs to run training for\")\n",
    "flags.DEFINE_integer(\"log_interval\", 5, \"Epochs between printing log info\")\n",
    "flags.DEFINE_integer(\"opt_steps\", 40, \"Optimization steps in each epoch\")\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T00:09:59.502910Z",
     "start_time": "2020-08-29T00:09:59.484687Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    '''Define Q-model'''\n",
    "\n",
    "    def __init__(self, num_bits, scope, reuse):\n",
    "        # initialize model\n",
    "        hidden_dim = 256\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            # ======================== TODO modify code ========================\n",
    "            # J: Goal conditioning brings two distinctions:\n",
    "            #    1. state has attached goal_state\n",
    "            #    2. reward is some distance to goal_state from current state\n",
    "            # Before goal conditioning:\n",
    "            #self.inp = tf.placeholder(shape=[None, num_bits], dtype=tf.float32)\n",
    "            self.inp = tf.placeholder(shape=[None, 2 * num_bits], dtype=tf.float32)\n",
    "\n",
    "            # ========================      END TODO       ========================\n",
    "            net = self.inp\n",
    "            net = slim.fully_connected(net, hidden_dim, activation_fn=tf.nn.relu)\n",
    "            self.out = slim.fully_connected(net, num_bits, activation_fn=None)\n",
    "            self.predict = tf.argmax(self.out, axis=1)\n",
    "            self.action_taken = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "            action_one_hot = tf.one_hot(self.action_taken, num_bits)\n",
    "            Q_val = tf.reduce_sum(self.out * action_one_hot, axis=1)\n",
    "            self.Q_target = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "            self.loss = tf.reduce_mean(tf.square(Q_val - self.Q_target))\n",
    "            self.train_step = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(self.loss)\n",
    "\n",
    "\n",
    "def update_target_graph(from_scope, to_scope, tau):\n",
    "    '''update the target network by copying over the weights from the policy\n",
    "    network to the target network'''\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "    ops = []\n",
    "    for (var1, var2) in zip(from_vars, to_vars):\n",
    "        ops.append(var2.assign(var2 * tau + (1 - tau) * var1))\n",
    "\n",
    "    return ops\n",
    "\n",
    "\n",
    "def updateTarget(ops, sess):\n",
    "    for op in ops:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T00:10:01.114377Z",
     "start_time": "2020-08-29T00:10:00.074037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jack/anaconda3/envs/cs330/lib/python3.7/site-packages/tf_slim/layers/layers.py:1898: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "# ************   Define global variables and initialize    ************ #\n",
    "\n",
    "num_bits = 7  # number of bits in the bit_flipping environment\n",
    "# \n",
    "tau = 0.95  # Polyak averaging parameter\n",
    "buffer_size = 1e6  # maximum number of elements in the replay buffer\n",
    "batch_size = 128  # number of samples to draw from the replay_buffer\n",
    "\n",
    "num_epochs = 150#FLAGS.num_epochs  # epochs to run training for\n",
    "num_episodes = 16  # episodes to run in the environment per epoch\n",
    "num_relabeled = 4  # relabeled experiences to add to replay_buffer each pass\n",
    "gamma = 0.98  # weighting past and future rewards\n",
    "\n",
    "# create bit flipping environment and replay buffer\n",
    "bit_env = BitFlipEnv(num_bits)\n",
    "replay_buffer = Buffer(buffer_size, batch_size)\n",
    "\n",
    "# set up Q-policy (model) and Q-target (target_model)\n",
    "model = Model(num_bits, scope='model', reuse=False)\n",
    "target_model = Model(num_bits, scope='target_model', reuse=False)\n",
    "\n",
    "update_ops_initial = update_target_graph('model', 'target_model', tau=0.0)\n",
    "update_ops = update_target_graph('model', 'target_model', tau=tau)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# start by making Q-target and Q-policy the same\n",
    "updateTarget(update_ops_initial, sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T00:10:44.631040Z",
     "start_time": "2020-08-29T00:10:44.607890Z"
    }
   },
   "outputs": [],
   "source": [
    "def solve_environment(state, goal_state, total_reward):\n",
    "    '''attempt to solve the bit flipping environment using the current policy'''\n",
    "\n",
    "    # list for recording what happened in the episode\n",
    "    episode_experience = []\n",
    "    succeeded = False\n",
    "\n",
    "    for t in range(num_bits):\n",
    "        # attempt to solve the state - number of steps given to solve the\n",
    "        # state is equal to the size of the vector\n",
    "\n",
    "        # ======================== TODO modify code ========================\n",
    "        #print(t, state, goal_state)\n",
    "        inp_state = state\n",
    "        # J: Concat goal_state to each state observation\n",
    "        inp_state = np.concatenate([state, goal_state])\n",
    "        #print(t, inp_state)\n",
    "        # forward pass to find action\n",
    "        action = sess.run(model.predict, feed_dict={model.inp: [inp_state]})[0]\n",
    "        # take the action\n",
    "        next_state, reward, done, _ = bit_env.step(action)\n",
    "        # J: In Goal cond. RL, reward=-distance(state, goal)\n",
    "        r_func = 'l2'\n",
    "        if r_func == 'sparse':\n",
    "            # -1 if not equal; 0 if equal\n",
    "            # -dirac{ state != goal_state }\n",
    "            reward = -1 * np.any(next_state != goal_state)\n",
    "        elif r_func == 'l2':\n",
    "            # L2 norm (squared): ||different bits||^2\n",
    "            reward = -1 * np.sum(np.power( (next_state - goal_state), 2) )\n",
    "\n",
    "        # J: Update state and next_state with goal_state (to sample from batch of experience later)\n",
    "        state_g = np.asarray([state, goal_state]).flatten()\n",
    "        next_state_g = np.asarray([next_state, goal_state]).flatten()\n",
    "        # add to the episode experience (what happened)\n",
    "        episode_experience.append((state_g, action, reward, next_state_g, goal_state))\n",
    "        # calculate total reward\n",
    "        total_reward += reward\n",
    "        # update state\n",
    "        state = next_state\n",
    "        # mark that we've finished the episode and succeeded with training\n",
    "        if done:\n",
    "            if succeeded:\n",
    "                continue\n",
    "            else:\n",
    "                succeeded = True\n",
    "\n",
    "        # ========================      END TODO       ========================\n",
    "\n",
    "    return succeeded, episode_experience, total_reward / FLAGS.log_interval\n",
    "\n",
    "\n",
    "def update_replay_buffer(episode_experience, HER):\n",
    "    '''adds past experience to the replay buffer. Training is done with episodes from the replay\n",
    "    buffer. When HER is used, relabeled experiences are also added to the replay buffer\n",
    "\n",
    "    inputs: epsidode_experience - list of transitions from the last episode\n",
    "    modifies: replay_buffer\n",
    "    outputs: None'''\n",
    "\n",
    "    for t in range(num_bits):\n",
    "        # copy actual experience from episode_experience to replay_buffer\n",
    "\n",
    "        # ======================== TODO modify code ========================\n",
    "        s, a, r, s_, g = episode_experience[t]\n",
    "        m = len(s) // 2\n",
    "        # state\n",
    "        inputs = s\n",
    "        # next state\n",
    "        inputs_ = s_\n",
    "        # add to the replay buffer\n",
    "        replay_buffer.add(inputs, a, r, inputs_)\n",
    "\n",
    "        # when HER is used, each call to update_replay_buffer should add num_relabeled\n",
    "        # relabeled points to the replay buffer\n",
    "\n",
    "        if HER == 'None':\n",
    "            # HER not being used, so do nothing\n",
    "            pass\n",
    "\n",
    "        elif HER == 'final':\n",
    "            # final - relabel based on final state in episode\n",
    "            # pass\n",
    "            _, _, _, final_state, g_ = episode_experience[-1]\n",
    "            new_goal = final_state[:m]\n",
    "            # Update next_state as [next_state, new_goal_state]\n",
    "            relabel_state = np.asarray([s_[:m], new_goal]).flatten()\n",
    "            # Update reward (distance)\n",
    "            r_new = -1 * np.sum(np.power((s_[:m] - new_goal), 2))\n",
    "            replay_buffer.add(inputs, a, r_new, relabel_state)\n",
    "\n",
    "        elif HER == 'future':\n",
    "            # future - relabel based on future state. At each timestep t, relabel the\n",
    "            # goal with a randomly select timestep between t and the end of the\n",
    "            # episode\n",
    "            # pass\n",
    "            t_future = np.random.randint(t, m)\n",
    "            _, _, _, relabel_goal, _ = episode_experience[t_future]\n",
    "            replay_buffer.add(inputs, a, r, relabel_goal)\n",
    "\n",
    "\n",
    "        elif HER == 'random':\n",
    "            # random - relabel based on a random state in the episode\n",
    "            #pass\n",
    "            m = len(episode_experience)\n",
    "            t_random = np.random.randint(0, m)\n",
    "            _, _, _, relabel_goal, _ = episode_experience[t_random]\n",
    "            replay_buffer.add(inputs, a, r, relabel_goal)\n",
    "\n",
    "        # ========================      END TODO       ========================\n",
    "\n",
    "        else:\n",
    "            print(\"Invalid value for Her flag - HER not used\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T00:10:45.033551Z",
     "start_time": "2020-08-29T00:10:45.029986Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_success_rate(success_rates, labels):\n",
    "    '''This function plots the success rate as a function of the number of cycles.\n",
    "    The results are averaged over num_epochs epochs.\n",
    "\n",
    "    inputs: success_rates - list with each element a list of success rates for\n",
    "                            a epochs of running flip_bits\n",
    "            labels - list of labels for each success_rate line'''\n",
    "\n",
    "    for i in range(len(success_rates)):\n",
    "        plt.plot(success_rates[i], label=labels[i])\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Success rate')\n",
    "    plt.title('Success rate with %d bits' % num_bits)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T00:10:45.469569Z",
     "start_time": "2020-08-29T00:10:45.457113Z"
    }
   },
   "outputs": [],
   "source": [
    "# ************   Main training loop    ************ #\n",
    "\n",
    "def flip_bits(HER=\"None\", \n",
    "              num_bits=7, \n",
    "              num_epochs=40,\n",
    "              log_interval=2,\n",
    "              opt_steps=40,\n",
    "             num_episodes=150):\n",
    "    '''Main loop for running in the bit flipping environment. The DQN is\n",
    "    trained over num_epochs. In each epoch, the agent runs in the environment\n",
    "    num_episodes number of times. The Q-target and Q-policy networks are\n",
    "    updated at the end of each epoch. Within one episode, Q-policy attempts\n",
    "    to solve the environment and is limited to the same number as steps as the\n",
    "    size of the environment\n",
    "\n",
    "    inputs: HER - string specifying whether to use HER'''\n",
    "\n",
    "    print(\"Running bit flip environment with %d bits and HER policy: %s\" % (num_bits, HER))\n",
    "\n",
    "    total_loss = []  # training loss for each epoch\n",
    "    success_rate = []  # success rate for each epoch\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        # Run for a fixed number of epochs\n",
    "\n",
    "        total_reward = 0.0  # total reward for the epoch\n",
    "        successes = []  # record success rate for each episode of the epoch\n",
    "        losses = []  # loss at the end of each epoch\n",
    "\n",
    "        for k in range(num_episodes):\n",
    "            # Run in the environment for num_episodes  \n",
    "\n",
    "            state, goal_state = bit_env.reset()  # reset the environment\n",
    "            # attempt to solve the environment\n",
    "            succeeded, episode_experience, total_reward = solve_environment(state, goal_state, total_reward)\n",
    "            successes.append(succeeded)  # track whether we succeeded in environment\n",
    "            update_replay_buffer(episode_experience, HER)  # add to the replay buffer; use specified  HER policy\n",
    "\n",
    "        for k in range(FLAGS.opt_steps):\n",
    "            # optimize the Q-policy network\n",
    "\n",
    "            # sample from the replay buffer\n",
    "            state, action, reward, next_state = replay_buffer.sample()\n",
    "            # forward pass through target network\n",
    "            target_net_Q = sess.run(target_model.out, feed_dict={target_model.inp: next_state})\n",
    "            # calculate target reward\n",
    "            target_reward = np.clip(np.reshape(reward, [-1]) + gamma * np.reshape(np.max(target_net_Q, axis=-1), [-1]),\n",
    "                                    -1. / (1 - gamma), 0)\n",
    "            # calculate loss\n",
    "            _, loss = sess.run([model.train_step, model.loss],\n",
    "                               feed_dict={model.inp: state, model.action_taken: np.reshape(action, [-1]),\n",
    "                                          model.Q_target: target_reward})\n",
    "            # append loss from this optimization step to the list of losses\n",
    "            losses.append(loss)\n",
    "\n",
    "        updateTarget(update_ops, sess)  # update target model by copying Q-policy to Q-target\n",
    "        success_rate.append(np.mean(successes))  # append mean success rate for this epoch\n",
    "\n",
    "        if i % FLAGS.log_interval == 0:\n",
    "            print('Epoch: %d  Mean reward: %f  Success rate: %.4f Mean loss: %.4f' % (\n",
    "                i, total_reward, np.mean(successes), np.mean(losses)))\n",
    "\n",
    "    return success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T00:10:45.940047Z",
     "start_time": "2020-08-29T00:10:45.848837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running bit flip environment with 7 bits and HER policy: final\n"
     ]
    },
    {
     "ename": "UnrecognizedFlagError",
     "evalue": "Unknown command line flag 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-7964d455cebf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msuccess_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflip_bits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHER\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'final'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-8c243db11870>\u001b[0m in \u001b[0;36mflip_bits\u001b[0;34m(HER, num_bits, num_epochs, log_interval, opt_steps, num_episodes)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbit_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# reset the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# attempt to solve the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0msucceeded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_experience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolve_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0msuccesses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msucceeded\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# track whether we succeeded in environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mupdate_replay_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_experience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHER\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# add to the replay buffer; use specified  HER policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-a73cb7ecc246>\u001b[0m in \u001b[0;36msolve_environment\u001b[0;34m(state, goal_state, total_reward)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# ========================      END TODO       ========================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msucceeded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_experience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_reward\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs330/lib/python3.7/site-packages/tensorflow_core/python/platform/flags.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# a flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m       \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs330/lib/python3.7/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, argv, known_only)\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0msuggestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_flag_suggestions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m       raise _exceptions.UnrecognizedFlagError(\n\u001b[0;32m--> 633\u001b[0;31m           name, value, suggestions=suggestions)\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m: Unknown command line flag 'f'"
     ]
    }
   ],
   "source": [
    "success_rate = flip_bits(HER='final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
